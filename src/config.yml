learning_model:
  source: 'model_defs/floor_model_attention.py'  # floor 전용 모델로 변경
  # 타겟 포인트 설정 (floor만 학습)
  # target_points: ['floor']  # ['center', 'floor', 'front', 'side'] 중 선택 가능 - floor_model_attention.py의 TARGET_POINTS 사용
  # 아키텍처 설정
  architecture:
    # FPD 아키텍처 사용 여부 (true: FPD 분류 기반 회귀, false: 기존 MLP)
    # use_fpd_architecture: true  # floor_model_attention.py의 USE_FPD_ARCHITECTURE 사용
    # 특징 추출 설정
    features:
      # image_size: [112, 112]  # floor_model_attention.py의 IMAGE_SIZE 사용
      # grid_size: 7  # floor_model_attention.py의 GRID_SIZE 사용
      # Autoencoder 기반 특성 추출 설정
      # use_autoencoder: false  # floor_model_attention.py의 USE_AUTOENCODER 사용
      # encoder_path: '../model/autoencoder_16x16_best.pth'  # floor_model_attention.py의 ENCODER_PATH 사용
      # encoder_latent_dim: 16  # floor_model_attention.py의 ENCODER_LATENT_DIM 사용
  # 모델 저장 설정
  checkpointing:
    save_dir: '../model'
    save_best_only: false
    save_frequency: 1000            # 에폭 단위
    # save_file_name: 'floor_attention'  # Floor only attention model - floor_model_attention.py의 SAVE_FILE_NAME 사용
    
# 데이터
data:
  source_folder: "../data/learning"  # 학습 데이터 경로
  labels_file: "labels.txt"  # 레이블 파일명
  test_id_suffix: '1'
  validation_ratio: 0.2
  random_seed: 42
  max_train_images: 200  # 학습 이미지 수 제한 (0이면 모두 사용, 100이면 100개만 사용)


# 학습 설정
training:
  batch_size: 512
  epochs: 100000000
  learning_rate: 0.001
  weight_decay: 0.01
  optimizer: 'adamw'  # adam, adamw, sgd
  gradient_clip: 5.0
  extract_features: true  # true면 특징을 미리 추출하여 메모리에 캐싱 (학습 속도 10-20배 향상)

  # 데이터 증강
  augmentation:
    enabled: true
    augment_count: 4  # 원본 데이터당 증강 샘플 수
    # 크롭 증강 설정
    crop:
      enabled: true
      min_ratio: 0.8  # 최소 크롭 비율 (80%)
      max_ratio: 1.0  # 최대 크롭 비율 (100%)
      max_shift: 0.15  # 최대 이동 비율 (15%)
    # 좌표 정규화 범위 설정 (112x112 이미지 기준)
    coordinate_range:
      x_min_ratio: -1.0   # x 최소: -112 (-width)
      x_max_ratio: 2.0    # x 최대: 224 (2*width)
      y_min_ratio: 0.0    # y 최소: 0
      y_max_ratio: 2.0    # y 최대: 224 (2*height)

  # 학습률 스케줄러
  scheduler:
    enabled: false  # 스케줄러를 완전히 비활성화 (고정 학습률 사용)
    type: 'step'  # 'reduce_on_plateau', 'step', 'cosine', 'none' 중 선택
                  # 'none'으로 설정하면 스케줄러 비활성화 (고정 학습률 사용)

    # ReduceLROnPlateau 설정 (type: 'reduce_on_plateau'일 때 사용)
    patience: 200        # reduce_on_plateau: 개선 없이 기다릴 에폭 수
    factor: 0.1         # reduce_on_plateau: 감소 비율

    # StepLR 설정 (type: 'step'일 때 사용)
    step_size: 10000    # step: 몇 에폭마다 감소
    gamma: 0.8          # step: 감소 비율 (새 LR = 기존 LR * gamma)

    # 시간 기반 LR 감소 (모든 스케줄러와 병행 사용 가능)
    time_based:
      enabled: true              # 시간 기반 감소 활성화
      patience_hours: 0.3        # Best 모델 갱신 없이 대기할 시간 (시간)
      factor: 0.7               # 감소 비율 (새 LR = 기존 LR * factor)

    # 공통 설정
    min_lr: 0.0000001   # 최소 학습률 (이 값 이하로 감소하지 않음)

  # Loss 설정
  loss:
    soft_label_sigma: 1.0      # Soft label Gaussian 분포의 표준편차
  
  # Early stopping
  early_stopping:
    patience: 999999999  # 사실상 비활성화
    min_delta: 0.00000001

  # 오차 분석 설정
  error_analysis:
    enabled: true               # 오차 분석 기능 활성화
    interval: 1000                 # 오차 분석 실행 간격 (에폭 단위)
    save_raw_data: true         # 오차 원천 데이터 저장 여부
    results_dir: '../result'   # 오차 데이터 저장 디렉토리

# 로깅 설정
logging:
  log_dir: '../logs'
